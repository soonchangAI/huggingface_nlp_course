{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177e2b73-1873-4c5c-b9a3-2bc7452f18ea",
   "metadata": {},
   "source": [
    "Transformers <code>pipeline()</code> function connects a model with necessary pre & post processing steps\n",
    "\n",
    "By default, the pipeline selects a certain pretrained model that has been finetuned for sentiment analysis in English. \n",
    "* Download the model and cache it\n",
    "* The cached model will be used & no need to redownload again\n",
    "\n",
    "Three main steps of handling input text:\n",
    "* Text is preprocessed into a format that model can understand\n",
    "* Pass preprocessed inputs to model\n",
    "* Predictions of the model are post-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498977ea-23f4-4d05-9a87-38323d4aa57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.66.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.24.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795739b8-64e4-4dc2-9979-4238dab8654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-26 21:00:12,931] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Using /home/cybertron/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/cybertron/.cache/torch_extensions/py310_cu117/cuda_kernel/build.ninja...\n",
      "Building extension module cuda_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Failed to load CUDA kernels. Mra requires custom CUDA kernels. Please verify that compatible versions of PyTorch and CUDA Toolkit are installed: Error building extension 'cuda_kernel'\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/TH -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cybertron/anaconda3/envs/llava/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -std=c++17 -c /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/kernels/mra/cuda_kernel.cu -o cuda_kernel.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mcuda_kernel.cuda.o \n",
      "/usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/TH -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cybertron/anaconda3/envs/llava/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -std=c++17 -c /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/kernels/mra/cuda_kernel.cu -o cuda_kernel.cuda.o \n",
      "/bin/sh: 1: /usr/local/cuda/bin/nvcc: not found\n",
      "[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/TH -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cybertron/anaconda3/envs/llava/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -std=c++17 -c /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/kernels/mra/cuda_launch.cu -o cuda_launch.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mcuda_launch.cuda.o \n",
      "/usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/TH -isystem /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cybertron/anaconda3/envs/llava/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -std=c++17 -c /home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/kernels/mra/cuda_launch.cu -o cuda_launch.cuda.o \n",
      "/bin/sh: 1: /usr/local/cuda/bin/nvcc: not found\n",
      "ninja: build stopped: subcommand failed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.7453370094299316}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I have been waiting 4 a huggingface course my entire life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fd64a9-7468-4ba2-8555-a3c651c6653b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.996371865272522},\n",
       " {'label': 'NEGATIVE', 'score': 0.998421311378479}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"I have been waiting for this delicious tuna sandwich my whole life\",\n",
    "             \"I hate pickles\"]\n",
    "classifier(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d0ee6-6fa5-41ca-8f9a-318977187245",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "* Allow us to specify desired labels for classification, without using labels of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735a14a7-bb92-4002-896a-58856596ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about making sandwich',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.6514201760292053, 0.25742191076278687, 0.09115791320800781]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "sentence = \"This is a course about making sandwich\"\n",
    "candidate_labels = [\"education\",\"politics\",\"business\"]\n",
    "\n",
    "classifier(sentence, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076168d-056e-4da3-9d62-fd5f499050fb",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "* Provide a prompt and the model will auto-complete it\n",
    "* May not have same result due to randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b67333-ed45-46cc-a073-f518630f481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this sandwich making class, we will teach you how to make sandwiches in a quick time.\\n\\nIf you have a big time interest in how to do most people's life as a few days away from home, here are some tips you should\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "prompt = \"In this sandwich making class, we will teach you how\"\n",
    "\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c45c4-ae90-4df0-8147-b1d7f9e1724a",
   "metadata": {},
   "source": [
    "Select a specific model from Hub to be used in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8b51aa-2b13-41c0-a09d-699eb6340e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this sandwich making class, we will teach you how to make an example of food. All the basic ingredients required are some simple ingredients and you will'},\n",
       " {'generated_text': 'In this sandwich making class, we will teach you how to learn different types of meatballs and make all of your breadballs from scratch. Learn how'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model='distilgpt2')\n",
    "prompt = \"In this sandwich making class, we will teach you how\"\n",
    "\n",
    "generator(prompt,max_length=30, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1687b-de62-45b6-9669-1e645c03e45c",
   "metadata": {},
   "source": [
    "## The Inference API\n",
    "* Test available models\n",
    "* Is a paid product\n",
    "\n",
    "## Mask filling\n",
    "Fill in the blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e681156-5303-4bfb-9a64-0731ebaf4bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19449160993099213,\n",
       "  'token': 442,\n",
       "  'token_str': ' making',\n",
       "  'sequence': 'This pasta making course will teach you about making spaghetti.'},\n",
       " {'score': 0.09701454639434814,\n",
       "  'token': 17798,\n",
       "  'token_str': ' homemade',\n",
       "  'sequence': 'This pasta making course will teach you about homemade spaghetti.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This pasta making course will teach you about <mask> spaghetti.\",top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5e429-81cc-4c5c-bb77-852332046e48",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "* Find which parts of input text corresponds to entities such as persons, locations, or organizations.\n",
    "* PER: person, ORG: organization, LOC: location\n",
    "* <code>grouped_entities=True</code> tells the pipeline to regroup together parts of the sentence that correspond to the same group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8edbe2-f6d9-4c20-9f6f-0a18c88c31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9986809,\n",
       "  'word': 'John',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9959449,\n",
       "  'word': 'Starbucks',\n",
       "  'start': 30,\n",
       "  'end': 39},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9956474,\n",
       "  'word': 'Georgetown',\n",
       "  'start': 43,\n",
       "  'end': 53}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "text = \"My name is John and I work at Starbucks in Georgetown\"\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca26cac3-a732-40f2-bdc1-903f4c2bac81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9986809,\n",
       "  'word': 'John',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9959449,\n",
       "  'word': 'Starbucks',\n",
       "  'start': 30,\n",
       "  'end': 39},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9956474,\n",
       "  'word': 'Georgetown',\n",
       "  'start': 43,\n",
       "  'end': 53}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(text, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572b881-9a8a-4d34-bb06-664d2b422442",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe92536-209b-4a7f-9c97-e6f553e17050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.8467773795127869, 'start': 0, 'end': 6, 'answer': 'Subway'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\")\n",
    "question = \"Where to can buy a sandwich?\"\n",
    "context = \"Subway is a sandwich joint from United States that sells delicious sandwich and cookies in Kuala Lumpur\"\n",
    "qa(question=question,context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb223f-bedd-49d8-9332-021ae749a535",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edebd98e-38c0-44a2-94e9-a78e80f0d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "text = \"\"\"Sandwich, in its basic form, slices of meat,  \n",
    "cheese, or other food placed between two slices of bread.\n",
    "Although this mode of consumption must be as old as meat \n",
    "and bread, the name was adopted only in the 18th century\n",
    "for John Montagu, 4th earl of Sandwich. According to an \n",
    "often-cited account from a contemporary French travel book,\n",
    "Sandwich had sliced meat and bread brought to him at the \n",
    "gaming table on one occasion so that he could continue \n",
    "to play as he ate; it seems more likely, however, \n",
    "that he ate these sandwiches as he worked at his desk or \n",
    "that the world became aware of them when he requested \n",
    "hem in London society. His title lent the preparation \n",
    "cachet, and soon it was fashionable to serve sandwiches\n",
    "on the European continent, and the word was incorporated \n",
    "into the French language. Since that time the \n",
    "sandwich has been incorporated into virtually every \n",
    "cuisine of the West by virtue of its simplicity of \n",
    "preparation, portability, and endless variety.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9477bf3a-f835-47b3-8cc9-686cd39400e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Sandwich, in its basic form, slices of meat, cheese, or other food placed between two slices of bread . The name was adopted only in the 18th century for John Montagu, 4th earl of Sandwich . His title lent the preparation, and soon it was fashionable to serve sandwiches on European continent .'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3956116-6900-4a2b-80ce-1cc439339891",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c197a84d-0bf5-4d10-9796-5af9cfa35522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'The sandwich is made with pineapple and bacon'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\",model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "text_fr = \"Le sandwich est fait avec des ananas et du bacon\"\n",
    "translator(text_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dcd92a-0def-434a-bfd9-fcf701fd7f70",
   "metadata": {},
   "source": [
    "## Bias and limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c1e26a-bf43-4d08-9da4-02a354ca816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "text1 = \"This man works as a [MASK].\"\n",
    "result = unmasker(text1)\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "text2 = \"This woman works as a [MASK].\"\n",
    "result = unmasker(text2)\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5eb12fa-5f7e-456d-81b0-1903f96cbec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fish', 'chicken', 'rice', 'beef', 'meat']\n"
     ]
    }
   ],
   "source": [
    "text3 = \"The best meal consists of egg, [MASK] and tuna.\"\n",
    "result = unmasker(text3)\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879f6c1-355a-4b76-b883-cdb1425474b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22318d-122f-4abe-a4aa-1b57b19707a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
