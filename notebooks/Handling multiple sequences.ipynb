{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e71c74-3efc-4626-bbd8-f058b456fa5c",
   "metadata": {},
   "source": [
    "Models expect a batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e842bdb-cb15-4f26-ab56-95c305d40e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-27 17:23:08,825] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence) \n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids]) # add one more dimension\n",
    "print(\"Input IDs:\", input_ids) \n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad2ff9c-0ddb-4833-8a53-be16f51e4006",
   "metadata": {},
   "source": [
    "Padding\n",
    "* Handling different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f6954c-3c72-4d68-bcd6-cb18ac054c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200]\n",
    "]\n",
    "\n",
    "padding_id = 100\n",
    "batched_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200,padding_id]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e217c6-4298-4ae6-a935-a68749cbe539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ad386-2509-4930-bb30-df4f24986410",
   "metadata": {},
   "source": [
    "Second row should be the same as the logits for second sentence. This is because of Transformer model's attention layers contextualize each token including the padding tokens. Need to tell attention layers to ignore the padding tokens by using attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1647324e-aaf6-45a8-8af8-efc4f93b3db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005c56e-4e72-4c98-a0fa-f9a3dc921a83",
   "metadata": {},
   "source": [
    "Longer sequences\n",
    "* Some model handle 512 or 1024 tokens\n",
    "* Would crash when process longer sequences\n",
    "* Use a model that supports longer sequence length\n",
    "* Truncate sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd3ccc-7e8e-4daf-bb7c-2d31c6162a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a2efd-f014-4c13-9def-6c489ec5a845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c2227-4c46-4590-9b05-b1f6a2dc9150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
