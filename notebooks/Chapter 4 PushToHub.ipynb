{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fb60bc-a078-440e-9490-f4745f8d8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b4d67-9f7f-4ba1-9d06-b16cbb7d0317",
   "metadata": {},
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbfc4e6-05ca-43c1-bd0f-afadd8d530ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963c0882fd4c48d29cfc36b8547ad722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acc4a0-ee34-455b-8658-09652bcf0b06",
   "metadata": {},
   "source": [
    "* Define a **TrainingArguments** class\n",
    "  * hyperparameters, directories etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d372c-1aa9-4e57-ada4-d945f84d96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "#training_args = TrainingArguments(\"test-trainer\")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271b9b7d-3e9a-4c04-bce9-cde0ede6d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-17 17:28:59,695] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc3eff-4fa6-40b9-af4b-2d184b7627f3",
   "metadata": {},
   "source": [
    "* A new classifier head is added, randomly iinitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb96d4c0-3c96-4e67-ba98-56a2e2199d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-finetuned\", save_strategy=\"epoch\", push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd83897-1e1a-4fc6-b7f5-d7ed84d9a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/soonchang/bert-finetuned into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args, \n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dc72b7-e5ac-4a1f-b099-808a8139187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoonchangpoh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/cybertron/fa54fcb6-b5e1-492e-978a-6389519c168a/huggingface_nlp_course/notebooks/wandb/run-20231217_173101-l4wnq8j6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/soonchangpoh/huggingface/runs/l4wnq8j6' target=\"_blank\">super-violet-17</a></strong> to <a href='https://wandb.ai/soonchangpoh/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/soonchangpoh/huggingface' target=\"_blank\">https://wandb.ai/soonchangpoh/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/soonchangpoh/huggingface/runs/l4wnq8j6' target=\"_blank\">https://wandb.ai/soonchangpoh/huggingface/runs/l4wnq8j6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [690/690 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.381200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=690, training_loss=0.30923866050830784, metrics={'train_runtime': 175.821, 'train_samples_per_second': 62.586, 'train_steps_per_second': 3.924, 'total_flos': 430291408824720.0, 'train_loss': 0.30923866050830784, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fb8d7-b908-4bec-bfe9-af5f672072d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359e8d8-38d7-45a2-a464-4fe69b5f427a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd91128-4e76-4379-9cb9-9ef480c90de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e6f3e-91ea-4411-8813-8f40d73461e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef95a6-5230-4fff-ab31-e68d9b7c0ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f9114-55e9-4809-9347-570a693206ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cf1bba-2754-4f03-b12d-3ee3804c3496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb25687d2484ba29118df3ceac61523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe273b9564946d882e8cdd217f86a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf96daf5a324b41a7365d713f73cf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7943e9df44af4102aacf92f786af44d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM , AutoTokenizer\n",
    "\n",
    "checkpoint = \"camembert-base\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45e061d0-3bda-43ac-a45c-e0e4e0cbd483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289b8b1715124dcbb3a0da0848434d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54c4ed6565040bba7db591274da7339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/soonchang/dummy-model/commit/92b8293e6b9f901134abe30c7dca0f37a8e81114', commit_message='Upload tokenizer', commit_description='', oid='92b8293e6b9f901134abe30c7dca0f37a8e81114', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"dummy-model\")\n",
    "tokenizer.push_to_hub(\"dummy-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a59f25-9385-418c-aedf-a1cf03305698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaec4aa-6054-4cd7-b2d6-b9a9d2437896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e78e6a-4874-479d-b93d-09ec978376e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde087c-618b-4581-947b-48492d8903b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946e77b-68e4-4242-b4ad-79b41dfb5682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178e788-822b-4fd3-9944-75f2544910cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac3-9771-4b16-b7b9-eff240363bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65a88169-7707-4b67-a20d-022d00e3f0c8",
   "metadata": {},
   "source": [
    "**compute_metric()**\n",
    "* Input: EvalPrediction (tuple with **predictions** & **label_ids**)\n",
    "* Return: Dictionary with key (name of metric) and val (float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d7f4e-957f-4bb0-862e-b236aee7bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets['validation'])\n",
    "print(predictions.predictions.shape)\n",
    "print(predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68753f-505f-405a-97ef-95ae32b2e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd11ae-8f14-415e-b5a5-2616ce33572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform predictions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0895d-a2a5-4975-a601-633aeaef3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install sklearn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bb8bc-4dad-4ca3-9c9e-0a3faa8aad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089a077-1180-4eb8-817d-131ea513500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe4226-2b51-41d6-97ee-ec0cb9006a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\",evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2570f4a-d611-4255-b2bc-9b627e32fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869bc6c-7e93-4c88-9c35-5904a15d4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c3fdb6-c8e5-40b4-8f8f-af8c97458ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import (\n",
    "    # User management\n",
    "    login,\n",
    "    logout,\n",
    "    whoami,\n",
    "\n",
    "    # Repository creation and management\n",
    "    create_repo,\n",
    "    delete_repo,\n",
    "    update_repo_visibility,\n",
    "\n",
    "    # And some methods to retrieve/change information about the content\n",
    "    list_models,\n",
    "    list_datasets,\n",
    "    list_metrics,\n",
    "    list_repo_files,\n",
    "    upload_file,\n",
    "    delete_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3109751e-7f6d-428b-ac0e-39de36aba775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/soonchang/dummy-test', endpoint='https://huggingface.co', repo_type='model', repo_id='soonchang/dummy-test')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "create_repo(\"dummy-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd6ded-9862-4c56-9aef-bfd4e6438799",
   "metadata": {},
   "source": [
    "Upload file\n",
    "\n",
    "* Method 1 **upload_file**\n",
    "   * doesnt require git & git-lfs\n",
    "   * Push using HTTP POST requests\n",
    "   * For <5GB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5752c40-3cd2-422f-94b9-0f045a5e448f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HfApi.upload_file() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upload_file\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/cybertron/fa54fcb6-b5e1-492e-978a-6389519c168a/huggingface_nlp_course/notebooks/bert-finetuned/config.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoonchang/dummy-test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1045\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: HfApi.upload_file() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import upload_file\n",
    "path = '/media/cybertron/fa54fcb6-b5e1-492e-978a-6389519c168a/huggingface_nlp_course/notebooks/bert-finetuned/config.json'\n",
    "upload_file(\n",
    "    path ,\n",
    "    path_in_repo=\"config.json\",\n",
    "    repo_id=\"soonchang/dummy-test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661807c7-e1a2-4fa1-b029-14f6601cb478",
   "metadata": {},
   "source": [
    "## Repository class\n",
    "* git like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0285e4b3-605d-49ac-a49b-6b8173019395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/soonchang/dummy-test into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(\"dummy-test\", clone_from=\"soonchang/dummy-test\")\n",
    "# create dummy-test in local working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33af0196-99ac-4c62-acee-f93efdc505c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure local clone is up to date\n",
    "repo.git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2958a39b-36bf-4e32-81f7-e71cf6a0d43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dummy_test/tokenizer_config.json',\n",
       " 'dummy_test/special_tokens_map.json',\n",
       " 'dummy_test/sentencepiece.bpe.model',\n",
       " 'dummy_test/added_tokens.json',\n",
       " 'dummy_test/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"dummy_test\")\n",
    "tokenizer.save_pretrained(\"dummy_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b758a2ff-ab76-4dc6-a9df-8934d2474832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98589fa3457a4080836ed054faa2b791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f938d00a72a040aca5ffeec2b3e9f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/soonchang/dummy-test\n",
      "   55bc536..de5c8e5  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/soonchang/dummy-test/commit/de5c8e550e35ee0898f211e7cf15dffd207a0475'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.git_add()\n",
    "repo.git_commit(\"Add model & tokenizer files\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db21663-1fec-42bd-976f-56f2ebdc89f2",
   "metadata": {},
   "source": [
    "* git-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818323a-6550-43dd-91e8-10396569d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db8309-cf53-4770-8fe0-61fa3c9e7cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
