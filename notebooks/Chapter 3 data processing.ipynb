{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb512250-56b6-4cff-8b89-e151b0cfd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# adding labels\n",
    "batch['labels'] = torch.tensor([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696f09e1-8b76-4c69-add3-d9787e0e3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import *\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss # calculate loss, ** batch passing dict\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6a9033e-17a6-4352-b0fa-dcfdafc7e0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset from Hub\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec690a1c-1e37-45d8-9b4f-71717e9b3484",
   "metadata": {},
   "source": [
    "* DatasetDict object with train, val, test set\n",
    "* Each contains several columns (sentence1, sentence 2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e06b4a0-32c9-4a18-bf58-057f67763154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Both Blair and Bush have faced accusations that they manipulated intelligence about weapons of mass destruction to make the case for military action .',\n",
       " 'sentence2': 'At home , the premier has faced accusations that he overplayed intelligence about weapons of mass destruction to make the case for war .',\n",
       " 'label': 1,\n",
       " 'idx': 450}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1797b00b-9634-4289-9dde-c24f5f384310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tell us about the features / each column\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d918c-6d65-4338-8a03-9903d0393833",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4de033-5fed-481f-b7e8-ac87ec4b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "# tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0498a83e-a248-4ea2-a661-13b17cbfa104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2117, 6251, 7987, 27225, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "[CLS] this is the first sentence. [SEP]\n",
      "[CLS] second sentence bruh. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"This is the first sentence.\",\"second sentence bruh.\"]\n",
    "inputs = tokenizer(inputs)\n",
    "print(inputs)\n",
    "for x in inputs['input_ids']:\n",
    "    print(tokenizer.decode(x)) # decode integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "623fda5e-b0d9-4a4a-9ad3-67e55666aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2117, 6251, 7987, 27225, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] this is the first sentence. [SEP] second sentence bruh. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\",\"second sentence bruh.\")\n",
    "print(inputs)\n",
    "\n",
    "print(tokenizer.decode(inputs['input_ids'])) # decode integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55e3743a-0044-4607-9449-a4c7db3e7d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'second',\n",
       " 'sentence',\n",
       " 'br',\n",
       " '##uh',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85e63da3-0c97-410b-874e-bb5c69a8e161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5557f20d-9d2e-4192-bea3-7c7a1e5ae7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "        raw_datasets['train']['sentence1'],\n",
    "        raw_datasets['train']['sentence2'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942402c-6d1c-4e70-9bdd-4c8d85b5e371",
   "metadata": {},
   "source": [
    "* disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists\n",
    "* return everything, only work if u have a large RAM to store whole dataset during the tokenization\n",
    "* Datasets library are Apache Arrow files stored on disk, only keep the samples u ask for loaded in memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4e43f-b3b8-486f-95e2-29328895ee7c",
   "metadata": {},
   "source": [
    "* Dataset.map() applies a function on each element of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "783dc655-b139-4225-aa19-86a951f48655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function, map gonna apply it to the stuff \n",
    "\n",
    "def tokenize_function(example):\n",
    "    processed = tokenizer(example['sentence1'],example['sentence2'], truncation=True)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841839b-c341-4fd5-97dc-c08343bc0e39",
   "metadata": {},
   "source": [
    "What it does?\n",
    "* Take a dictionary & returns a new dictionary with keys input_ids, attention_mask\n",
    "* Also works if example dictionary contains several samples / Each key is a list of sentences\n",
    "    * Can use batched=True\n",
    "* Leave padding out, because not efficient\n",
    "  * Better to pad when building batch  (pad to that batch's maximum length\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8629995-91cc-48d9-aead-680453787489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 3668/3668 [00:00<00:00, 22286.37 examples/s]\n",
      "Map: 100%|██████████████████████████| 408/408 [00:00<00:00, 17303.62 examples/s]\n",
      "Map: 100%|████████████████████████| 1725/1725 [00:00<00:00, 22482.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function, \n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "015f3cf8-71a6-4b94-a820-b2dbb79c00f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad768e-1308-4809-8fd4-ac4d5a579b8e",
   "metadata": {},
   "source": [
    "* Observe that new field as new key\n",
    "* Can use multiprocessing , set **num_proc**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9d161-57e4-4e82-9592-535d80b38768",
   "metadata": {},
   "source": [
    "* Collate function - put samples inside a batch\n",
    "  * Default is a function that converts samples to tensors & concat\n",
    "* Pass as an argument to **DataLoader**\n",
    "* Do padding here to avoid over-long input, leads to speedup\n",
    "  * Except for TPUs that prefer fixed shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b065e82d-d7cf-4378-9bca-b9bf5757541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# takes tokenizer as arg, to find out the pad token \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c453920-b38c-4528-b8af-b9643ae04583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets['train'][:8]\n",
    "samples = {k:v for k,v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "# dont need idx, s1 and s2\n",
    "[len(x) for x in samples[\"input_ids\"]] # length of samples of this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1787d6a-f57d-4e72-a502-f4595990599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k:v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f052d-d515-42ab-8d64-7cd279477f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
