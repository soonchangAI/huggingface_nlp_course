{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fedeec9-5b98-4d71-900f-af05ada85daa",
   "metadata": {},
   "source": [
    "<code>AutoModel</code>\n",
    "* Instantiate any model from a checkpoint\n",
    "* Wrappers over available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f1fabb-7624-4e5e-8bb7-08891c90a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-27 15:54:29,544] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "# build config\n",
    "config = BertConfig()\n",
    "# Build model from config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4359018-379b-4352-8b81-f5e5ea35133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e9fe3-3d3f-4f09-9b75-182dfb0e9800",
   "metadata": {},
   "source": [
    "Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f33a562-7a62-482a-b999-92e9206bfbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "model.safetensors:   0%|                             | 0.00/436M [00:00<?, ?B/s]\u001b[A\n",
      "model.safetensors:   2%|▍                   | 10.5M/436M [00:01<01:01, 6.94MB/s]\u001b[A\n",
      "model.safetensors:   5%|▉                   | 21.0M/436M [00:04<01:28, 4.68MB/s]\u001b[A\n",
      "model.safetensors:   7%|█▍                  | 31.5M/436M [00:07<01:36, 4.17MB/s]\u001b[A\n",
      "model.safetensors:  10%|█▉                  | 41.9M/436M [00:10<01:41, 3.88MB/s]\u001b[A\n",
      "model.safetensors:  12%|██▍                 | 52.4M/436M [00:13<01:41, 3.76MB/s]\u001b[A\n",
      "model.safetensors:  14%|██▉                 | 62.9M/436M [00:16<01:41, 3.67MB/s]\u001b[A\n",
      "model.safetensors:  17%|███▎                | 73.4M/436M [00:18<01:39, 3.63MB/s]\u001b[A\n",
      "model.safetensors:  19%|███▊                | 83.9M/436M [00:21<01:37, 3.59MB/s]\u001b[A\n",
      "model.safetensors:  22%|████▎               | 94.4M/436M [00:24<01:35, 3.58MB/s]\u001b[A\n",
      "model.safetensors:  24%|█████                | 105M/436M [00:27<01:32, 3.56MB/s]\u001b[A\n",
      "model.safetensors:  26%|█████▌               | 115M/436M [00:30<01:29, 3.56MB/s]\u001b[A\n",
      "model.safetensors:  29%|██████               | 126M/436M [00:33<01:27, 3.55MB/s]\u001b[A\n",
      "model.safetensors:  31%|██████▌              | 136M/436M [00:39<01:50, 2.72MB/s]\u001b[A\n",
      "model.safetensors:  34%|███████              | 147M/436M [00:41<01:29, 3.21MB/s]\u001b[A\n",
      "model.safetensors:  36%|███████▌             | 157M/436M [00:44<01:22, 3.37MB/s]\u001b[A\n",
      "model.safetensors:  39%|████████             | 168M/436M [00:47<01:17, 3.44MB/s]\u001b[A\n",
      "model.safetensors:  41%|████████▌            | 178M/436M [00:50<01:14, 3.48MB/s]\u001b[A\n",
      "model.safetensors:  43%|█████████            | 189M/436M [00:53<01:10, 3.49MB/s]\u001b[A\n",
      "model.safetensors:  46%|█████████▌           | 199M/436M [00:56<01:07, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  48%|██████████           | 210M/436M [00:59<01:04, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  51%|██████████▌          | 220M/436M [01:02<01:01, 3.52MB/s]\u001b[A\n",
      "model.safetensors:  53%|███████████          | 231M/436M [01:05<00:58, 3.52MB/s]\u001b[A\n",
      "model.safetensors:  55%|███████████▌         | 241M/436M [01:08<00:55, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  58%|████████████▏        | 252M/436M [01:11<00:52, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  60%|████████████▋        | 262M/436M [01:13<00:49, 3.54MB/s]\u001b[A\n",
      "model.safetensors:  63%|█████████████▏       | 273M/436M [01:16<00:46, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  65%|█████████████▋       | 283M/436M [01:19<00:43, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  67%|██████████████▏      | 294M/436M [01:22<00:40, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  70%|██████████████▋      | 304M/436M [01:25<00:37, 3.54MB/s]\u001b[A\n",
      "model.safetensors:  72%|███████████████▏     | 315M/436M [01:28<00:34, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  75%|███████████████▋     | 325M/436M [01:31<00:31, 3.54MB/s]\u001b[A\n",
      "model.safetensors:  77%|████████████████▏    | 336M/436M [01:34<00:28, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  79%|████████████████▋    | 346M/436M [01:37<00:25, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  82%|█████████████████▏   | 357M/436M [01:40<00:22, 3.53MB/s]\u001b[A\n",
      "model.safetensors:  84%|█████████████████▋   | 367M/436M [01:43<00:19, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  87%|██████████████████▏  | 377M/436M [01:46<00:16, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  89%|██████████████████▋  | 388M/436M [01:49<00:13, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  91%|███████████████████▏ | 398M/436M [01:52<00:10, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  94%|███████████████████▋ | 409M/436M [01:55<00:07, 3.52MB/s]\u001b[A\n",
      "model.safetensors:  96%|████████████████████▏| 419M/436M [01:58<00:04, 3.52MB/s]\u001b[A\n",
      "model.safetensors:  99%|████████████████████▋| 430M/436M [02:01<00:01, 3.53MB/s]\u001b[A\n",
      "model.safetensors: 100%|█████████████████████| 436M/436M [02:03<00:00, 3.54MB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fdc769-8850-446b-8b93-da4a5793b522",
   "metadata": {},
   "source": [
    "* The weights are cached at <code>~/.cache/huggingface/transformers</code>\n",
    "\n",
    "* Future call of <code>from_pretrained()</code> will not download it again\n",
    "\n",
    "* The cache folder location can be customized by setting <code>HF_HOME</code> environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04545485-dab6-4ae6-9eeb-62bcb3bd0e7a",
   "metadata": {},
   "source": [
    "## Saving methods\n",
    "* <code>config.json</code>: archiecture info / attributes for model building and metadata\n",
    "* <code>pytorch_model.bin</code>: state dictionary that contains model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4d73a3-b104-4a99-b4a7-3a0590ad11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"save_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee088d1-5ffb-44e9-8519-9a4325ca9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls save_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca94319-bb23-4caa-b4f3-75f12c2bad73",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e9f84c-6d0e-48bd-8e76-d3265b7a20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'was', 'a', 'chef']\n"
     ]
    }
   ],
   "source": [
    "# split by whitespace\n",
    "\n",
    "text = \"John was a chef\"\n",
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4db6f6-f6a0-4558-a27a-82f7ad72907b",
   "metadata": {},
   "source": [
    "* A vocabulary contains tokens from corpus. Each word is assigned an ID, from 0 to size of vocabulary\n",
    "* If use a word-based tokenizer\n",
    "    * \"dog\" are represented differently than \"dogs\"\n",
    "    * Initially, model does not know \"dog\" and \"dog\" are similar\n",
    "    * Words not in vocabulary are represented with unknown token [UNK]\n",
    "  * Bad to have many words tokenized into [UNK]\n",
    "* The solution is using character-based tokenization\n",
    "  * Much smaller vocabulary\n",
    "  * Fewer out-of-vocabulary (unknown) tokens because every word can be built with characters\n",
    "\n",
    "* Character-based tokenization is not perfect either\n",
    "  *  Large amount of tokens to be processed, whereas a word would only be a token with a word-based tokenizer\n",
    "  * Some argue characters are less meaningful\n",
    "    * Depends on the language, a chinese character carries more information than a character in Latin language.\n",
    "* The best of both worlds, subword tokenization combines both approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7994cf-e769-497e-9a65-005dbd950250",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "* Frequently used words should not be split into subwords\n",
    "* Rare words should be decomposed into meaningful subwords\n",
    "* For example, tokenization is split into \"token\" and \"ization\". Both contains semantic meaning.\n",
    "    *  Useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords\n",
    "* Byte-level BPE (GPT-2), WordPiece (BERT), SentencePiece or Unigram (multilingual model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68011c9f-4bf6-4727-829b-6196141ec57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b35e89-371c-469a-89aa-11ef7bb20e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizer_config.json: 100%|█████████████████| 29.0/29.0 [00:00<00:00, 37.2kB/s]\u001b[A\n",
      "\n",
      "vocab.txt:   0%|                                     | 0.00/213k [00:00<?, ?B/s]\u001b[A\n",
      "vocab.txt: 100%|██████████████████████████████| 213k/213k [00:00<00:00, 418kB/s]\u001b[A\n",
      "\n",
      "tokenizer.json:   0%|                                | 0.00/436k [00:00<?, ?B/s]\u001b[A\n",
      "tokenizer.json: 100%|████████████████████████| 436k/436k [00:00<00:00, 1.30MB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d043ad6e-fb1d-4e9e-bd19-3d03787e0620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1606, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12da4ef5-3c84-486d-ae8c-44bb9371d0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('save_tokenizer/tokenizer_config.json',\n",
       " 'save_tokenizer/special_tokens_map.json',\n",
       " 'save_tokenizer/vocab.txt',\n",
       " 'save_tokenizer/added_tokens.json',\n",
       " 'save_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "\n",
    "tokenizer.save_pretrained(\"save_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc51e9b-44dc-49cc-9a1c-ba10f5c61f8b",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "* Two steps: (a) Tokenization (b) conversion to input IDs\n",
    "* Tokenization: split text into words (tokens)\n",
    "    * Multiple rules\n",
    "* Conversion to numbers\n",
    "    * <code>Tokenizer</code> has vocabulary, downloaded when instantiated with from.pretrained()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c27cc4fe-d0d4-4de1-97a1-71c81493df93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364f3bab-1dd2-4464-bafe-cc8119e0e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1606, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674b9bd-6908-46c1-977e-af42d06473c8",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "* Converts indices back to tokens\n",
    "* Group together tokens that are part of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c4d4e1-3afd-4472-bb5d-e039170c158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a Transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899f6c3-2f1f-4f2b-a190-9ebab8df5cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfa4fe-4150-4ed5-954d-1197e70e84b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0199d53-a4d0-46a9-929b-0c0dd9d8ebd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
